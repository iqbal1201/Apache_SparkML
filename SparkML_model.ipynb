{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML Model Development\n",
    "\n",
    "This project aims to create model using SparkML, a library that run on top of Apache Spark. SparkML is suitable for creating a model that learn from very big dataset (> millions dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes, MultilayerPerceptronClassifier,DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkML Model Development\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Doing Data Preprocessing\n",
    "\n",
    "Let's import the dataset first, removing null values, and casting the data type into correct data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "data = spark.read.options(header='True', inferSchema=True, delimiter=',').csv(\"D:\\Self_project\\sparkml_classification\\data\\dataset_2millions.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---+-----+----+-----+---------+----------+---------+-------+\n",
      "| pointid|grid_code|Red|Green|Blue|Label| norm_red|norm_green|norm_blue|Label_1|\n",
      "+--------+---------+---+-----+----+-----+---------+----------+---------+-------+\n",
      "|19300985|        1|0.0| 43.0|20.0|    1|0,0941176| 0,1686275|0,0784314|      1|\n",
      "| 5665222|        0|0.0| 69.0|37.0|    0|0,1764706| 0,2705882|0,1450980|      0|\n",
      "|34365418|        0|0.0| 47.0|25.0|    0|0,1176471| 0,1843137|0,0980392|      0|\n",
      "|13371923|        0|0.0|116.0|89.0|    0|0,4470588| 0,4549020|0,3490196|      0|\n",
      "|10244697|        0|0.0| 57.0|40.0|    0|0,1568628| 0,2235294|0,1568628|      0|\n",
      "|79029168|        0|0.0| 49.0|21.0|    0|0,0941176| 0,1921569|0,0823529|      0|\n",
      "|20882365|        0|0.0| 51.0|26.0|    0|0,1607843| 0,2000000|0,1019608|      0|\n",
      "| 2966066|        0|0.0| 94.0|61.0|    0|0,2196078| 0,3686275|0,2392157|      0|\n",
      "|29171445|        1|0.0| 44.0|21.0|    1|0,0980392| 0,1725490|0,0823529|      1|\n",
      "|44246354|        0|0.0| 53.0|29.0|    0|0,1333333| 0,2078431|0,1137255|      0|\n",
      "|23969154|        1|0.0| 54.0|33.0|    1|0,1254902| 0,2117647|0,1294118|      1|\n",
      "|10592976|        1|0.0| 51.0|31.0|    1|0,1215686| 0,2000000|0,1215686|      1|\n",
      "|19015960|        1|0.0| 49.0|32.0|    1|0,1215686| 0,1921569|0,1254902|      1|\n",
      "|33784291|        0|0.0| 68.0|32.0|    0|0,2039216| 0,2666667|0,1254902|      0|\n",
      "|80038518|        0|0.0| 65.0|31.0|    0|0,1450980| 0,2549020|0,1215686|      0|\n",
      "|13009722|        0|1.0|140.0|87.0|    0|0,5098040| 0,5490196|0,3411765|      0|\n",
      "|44738434|        0|0.0| 80.0|50.0|    0|0,3529412| 0,3137255|0,1960784|      0|\n",
      "|26933271|        1|0.0| 93.0|84.0|    1|0,3019608| 0,3647059|0,3294118|      1|\n",
      "|54832083|        1|0.0| 40.0|20.0|    1|0,0862745| 0,1568628|0,0784314|      1|\n",
      "|20971155|        0|0.0| 90.0|72.0|    0|0,3215686| 0,3529412|0,2823530|      0|\n",
      "+--------+---------+---+-----+----+-----+---------+----------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+-----+\n",
      "| norm_red|norm_green|norm_blue|Label|\n",
      "+---------+----------+---------+-----+\n",
      "|0,0941176| 0,1686275|0,0784314|    1|\n",
      "|0,1764706| 0,2705882|0,1450980|    0|\n",
      "|0,1176471| 0,1843137|0,0980392|    0|\n",
      "|0,4470588| 0,4549020|0,3490196|    0|\n",
      "|0,1568628| 0,2235294|0,1568628|    0|\n",
      "|0,0941176| 0,1921569|0,0823529|    0|\n",
      "|0,1607843| 0,2000000|0,1019608|    0|\n",
      "|0,2196078| 0,3686275|0,2392157|    0|\n",
      "|0,0980392| 0,1725490|0,0823529|    1|\n",
      "|0,1333333| 0,2078431|0,1137255|    0|\n",
      "|0,1254902| 0,2117647|0,1294118|    1|\n",
      "|0,1215686| 0,2000000|0,1215686|    1|\n",
      "|0,1215686| 0,1921569|0,1254902|    1|\n",
      "|0,2039216| 0,2666667|0,1254902|    0|\n",
      "|0,1450980| 0,2549020|0,1215686|    0|\n",
      "|0,5098040| 0,5490196|0,3411765|    0|\n",
      "|0,3529412| 0,3137255|0,1960784|    0|\n",
      "|0,3019608| 0,3647059|0,3294118|    1|\n",
      "|0,0862745| 0,1568628|0,0784314|    1|\n",
      "|0,3215686| 0,3529412|0,2823530|    0|\n",
      "+---------+----------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only meaningful columns\n",
    "data = data.select(\"norm_red\", \"norm_green\", \"norm_blue\", \"Label\")\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+-----+\n",
      "|norm_red|norm_green|norm_blue|Label|\n",
      "+--------+----------+---------+-----+\n",
      "|       0|         0|        0|    0|\n",
      "+--------+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the null value\n",
    "data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('norm_red', 'string'),\n",
       " ('norm_green', 'string'),\n",
       " ('norm_blue', 'string'),\n",
       " ('Label', 'int')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data type\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('norm_red', 'double'),\n",
       " ('norm_green', 'double'),\n",
       " ('norm_blue', 'double'),\n",
       " ('Label', 'int')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is issue with our datatype column, it is supposed to be a double but turns out a string\n",
    "# We need to convert it into double\n",
    "# Before convert it into double, replace the \",\" sign into \".\" because double type only can read the \".\" sign\n",
    "\n",
    "\n",
    "data = data.withColumn(\"norm_red\", regexp_replace('norm_red', ',', '.'))\n",
    "data = data.withColumn(\"norm_red\", data.norm_red.cast(DoubleType()))\n",
    "\n",
    "data = data.withColumn(\"norm_green\", regexp_replace('norm_green', ',', '.'))\n",
    "data = data.withColumn(\"norm_green\", data.norm_red.cast(DoubleType()))\n",
    "\n",
    "data = data.withColumn(\"norm_blue\", regexp_replace('norm_blue', ',', '.'))\n",
    "data = data.withColumn(\"norm_blue\", data.norm_red.cast(DoubleType()))\n",
    "\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+-----+\n",
      "| norm_red|norm_green|norm_blue|Label|\n",
      "+---------+----------+---------+-----+\n",
      "|0.0941176| 0.0941176|0.0941176|    1|\n",
      "|0.1764706| 0.1764706|0.1764706|    0|\n",
      "|0.1176471| 0.1176471|0.1176471|    0|\n",
      "|0.4470588| 0.4470588|0.4470588|    0|\n",
      "|0.1568628| 0.1568628|0.1568628|    0|\n",
      "|0.0941176| 0.0941176|0.0941176|    0|\n",
      "|0.1607843| 0.1607843|0.1607843|    0|\n",
      "|0.2196078| 0.2196078|0.2196078|    0|\n",
      "|0.0980392| 0.0980392|0.0980392|    1|\n",
      "|0.1333333| 0.1333333|0.1333333|    0|\n",
      "|0.1254902| 0.1254902|0.1254902|    1|\n",
      "|0.1215686| 0.1215686|0.1215686|    1|\n",
      "|0.1215686| 0.1215686|0.1215686|    1|\n",
      "|0.2039216| 0.2039216|0.2039216|    0|\n",
      "| 0.145098|  0.145098| 0.145098|    0|\n",
      "| 0.509804|  0.509804| 0.509804|    0|\n",
      "|0.3529412| 0.3529412|0.3529412|    0|\n",
      "|0.3019608| 0.3019608|0.3019608|    1|\n",
      "|0.0862745| 0.0862745|0.0862745|    1|\n",
      "|0.3215686| 0.3215686|0.3215686|    0|\n",
      "+---------+----------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Dataset Before Model Training\n",
    "\n",
    "Vectorize the dataset is key important thing before we train our model in Spark ML. It involves converting data into a format that machine learning algorithms can efficiently process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['norm_red', 'norm_green', 'norm_blue']  # Replace with your actual feature columns\n",
    "label = 'Label'\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "data = data.select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.0941176,0.0941...|    1|\n",
      "|[0.1764706,0.1764...|    0|\n",
      "|[0.1176471,0.1176...|    0|\n",
      "|[0.4470588,0.4470...|    0|\n",
      "|[0.1568628,0.1568...|    0|\n",
      "|[0.0941176,0.0941...|    0|\n",
      "|[0.1607843,0.1607...|    0|\n",
      "|[0.2196078,0.2196...|    0|\n",
      "|[0.0980392,0.0980...|    1|\n",
      "|[0.1333333,0.1333...|    0|\n",
      "|[0.1254902,0.1254...|    1|\n",
      "|[0.1215686,0.1215...|    1|\n",
      "|[0.1215686,0.1215...|    1|\n",
      "|[0.2039216,0.2039...|    0|\n",
      "|[0.145098,0.14509...|    0|\n",
      "|[0.509804,0.50980...|    0|\n",
      "|[0.3529412,0.3529...|    0|\n",
      "|[0.3019608,0.3019...|    1|\n",
      "|[0.0862745,0.0862...|    1|\n",
      "|[0.3215686,0.3215...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: LogisticRegression_83c8a4cc17d8\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'predictionCol'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m cvModel\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Evaluate metrics\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     metrics_values \u001b[38;5;241m=\u001b[39m [evaluate_metrics(predictions, metric) \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics]\n\u001b[0;32m     57\u001b[0m     model_results\u001b[38;5;241m.\u001b[39mappend(metrics_values)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Compute average metrics across folds\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[38], line 56\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     53\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m cvModel\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Evaluate metrics\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     metrics_values \u001b[38;5;241m=\u001b[39m [\u001b[43mevaluate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics]\n\u001b[0;32m     57\u001b[0m     model_results\u001b[38;5;241m.\u001b[39mappend(metrics_values)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Compute average metrics across folds\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[38], line 18\u001b[0m, in \u001b[0;36mevaluate_metrics\u001b[1;34m(predictions, metric)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_metrics\u001b[39m(predictions, metric):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 18\u001b[0m         evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mBinaryClassificationEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictionCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetricName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m evaluator\u001b[38;5;241m.\u001b[39mevaluate(predictions)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\mjanuadi\\.conda\\envs\\sparkml\\lib\\site-packages\\pyspark\\__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'predictionCol'"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "models = [\n",
    "    LogisticRegression(featuresCol='features', labelCol='label'),\n",
    "    RandomForestClassifier(featuresCol='features', labelCol='label'),\n",
    "    GBTClassifier(featuresCol='features', labelCol='label'),\n",
    "    DecisionTreeClassifier(featuresCol='features', labelCol='label'),\n",
    "    NaiveBayes(featuresCol='features', labelCol='label'),\n",
    "    MultilayerPerceptronClassifier(featuresCol='features', labelCol='label', layers=[len(feature_columns), 2, 2]),  # Example layers\n",
    "    LinearSVC(featuresCol='features', labelCol='label')\n",
    "]\n",
    "\n",
    "# Define evaluation metrics\n",
    "metrics = ['accuracy', 'recall', 'precision', 'f1', 'iou']\n",
    "\n",
    "# Define the evaluator function for each metric\n",
    "def evaluate_metrics(predictions, metric):\n",
    "    if metric == 'accuracy':\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        return evaluator.evaluate(predictions)\n",
    "    elif metric == 'recall':\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "        return evaluator.evaluate(predictions)\n",
    "    elif metric == 'precision':\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "        return evaluator.evaluate(predictions)\n",
    "    elif metric == 'f1':\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "        return evaluator.evaluate(predictions)\n",
    "    elif metric == 'iou':\n",
    "        # Assuming predictions contain rawPrediction column\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "        return evaluator.evaluate(predictions)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Define param grid (optional, for hyperparameter tuning)\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "# Train and evaluate models\n",
    "results = []\n",
    "for model in models:\n",
    "    print(f\"Evaluating model: {model}\")\n",
    "    model_results = []\n",
    "    for i in range(5):  # 5-fold cross-validation\n",
    "        # Train the model\n",
    "        crossval = CrossValidator(estimator=model,\n",
    "                                  estimatorParamMaps=paramGrid,\n",
    "                                  evaluator=BinaryClassificationEvaluator(),\n",
    "                                  numFolds=2)\n",
    "        cvModel = crossval.fit(data)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = cvModel.transform(data)\n",
    "        \n",
    "        # Evaluate metrics\n",
    "        metrics_values = [evaluate_metrics(predictions, metric) for metric in metrics]\n",
    "        model_results.append(metrics_values)\n",
    "    \n",
    "    # Compute average metrics across folds\n",
    "    avg_metrics = [sum(metric) / len(metric) for metric in zip(*model_results)]\n",
    "    results.append(avg_metrics)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results, columns=metrics)\n",
    "\n",
    "# Write results to CSV file\n",
    "results_df.to_csv(\"metrics_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: LogisticRegression_9246039ec13d\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import (LogisticRegression, RandomForestClassifier, \n",
    "                                       GBTClassifier, DecisionTreeClassifier, \n",
    "                                       NaiveBayes, MultilayerPerceptronClassifier, LinearSVC)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "\n",
    "# Define models\n",
    "models = [\n",
    "    LogisticRegression(featuresCol='features', labelCol='label'),\n",
    "    RandomForestClassifier(featuresCol='features', labelCol='label'),\n",
    "    GBTClassifier(featuresCol='features', labelCol='label'),\n",
    "    DecisionTreeClassifier(featuresCol='features', labelCol='label'),\n",
    "    NaiveBayes(featuresCol='features', labelCol='label'),\n",
    "    MultilayerPerceptronClassifier(featuresCol='features', labelCol='label', layers=[len(feature_columns), 2, 2]),  # Example layers\n",
    "    LinearSVC(featuresCol='features', labelCol='label')\n",
    "]\n",
    "\n",
    "# Define evaluation metrics\n",
    "metrics = ['accuracy', 'recall', 'precision', 'f1', 'areaUnderROC', 'iou']\n",
    "\n",
    "# Define the evaluator function for each metric\n",
    "def evaluate_metrics(predictions, metric):\n",
    "    if metric == 'accuracy':\n",
    "        total_count = predictions.count()\n",
    "        if total_count == 0:\n",
    "            return 0.0\n",
    "        correct_predictions = predictions.filter(predictions.label == predictions.prediction).count()\n",
    "        accuracy = correct_predictions / float(total_count)\n",
    "        return accuracy\n",
    "    elif metric == 'recall':\n",
    "        tp = predictions.filter((predictions.label == 1) & (predictions.prediction == 1)).count()\n",
    "        fn = predictions.filter((predictions.label == 1) & (predictions.prediction == 0)).count()\n",
    "        if tp + fn == 0:\n",
    "            return 0.0\n",
    "        recall = tp / float(tp + fn)\n",
    "        return recall\n",
    "    elif metric == 'precision':\n",
    "        tp = predictions.filter((predictions.label == 1) & (predictions.prediction == 1)).count()\n",
    "        fp = predictions.filter((predictions.label == 0) & (predictions.prediction == 1)).count()\n",
    "        if tp + fp == 0:\n",
    "            return 0.0\n",
    "        precision = tp / float(tp + fp)\n",
    "        return precision\n",
    "    elif metric == 'f1':\n",
    "        tp = predictions.filter((predictions.label == 1) & (predictions.prediction == 1)).count()\n",
    "        fp = predictions.filter((predictions.label == 0) & (predictions.prediction == 1)).count()\n",
    "        fn = predictions.filter((predictions.label == 1) & (predictions.prediction == 0)).count()\n",
    "        if tp + fp == 0 or tp + fn == 0:\n",
    "            return 0.0\n",
    "        precision = tp / float(tp + fp)\n",
    "        recall = tp / float(tp + fn)\n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "    elif metric == 'areaUnderROC':\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "        return evaluator.evaluate(predictions)\n",
    "    elif metric == 'iou':\n",
    "        tp = predictions.filter((predictions.label == 1) & (predictions.prediction == 1)).count()\n",
    "        fp = predictions.filter((predictions.label == 0) & (predictions.prediction == 1)).count()\n",
    "        fn = predictions.filter((predictions.label == 1) & (predictions.prediction == 0)).count()\n",
    "        if tp + fp + fn == 0:\n",
    "            return 0.0\n",
    "        iou = tp / float(tp + fp + fn)\n",
    "        return iou\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Define param grid (optional, for hyperparameter tuning)\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "# Train and evaluate models\n",
    "results = []\n",
    "for model in models:\n",
    "    print(f\"Evaluating model: {model}\")\n",
    "    model_results = []\n",
    "    for i in range(10):  # 5-fold cross-validation\n",
    "        # Train the model\n",
    "        crossval = CrossValidator(estimator=model,\n",
    "                                  estimatorParamMaps=paramGrid,\n",
    "                                  evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderROC\"),\n",
    "                                  numFolds=10)\n",
    "        cvModel = crossval.fit(data)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = cvModel.transform(data)\n",
    "        \n",
    "        # Evaluate metrics\n",
    "        metrics_values = {metric: evaluate_metrics(predictions, metric) for metric in metrics}\n",
    "        model_results.append(metrics_values)\n",
    "    \n",
    "    results.append(model_results)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_flattened = []\n",
    "for model_index, model_results in enumerate(results):\n",
    "    for fold_index, fold_metrics in enumerate(model_results):\n",
    "        fold_metrics['model'] = model_index\n",
    "        fold_metrics['fold'] = fold_index\n",
    "        results_flattened.append(fold_metrics)\n",
    "\n",
    "results_df = pd.DataFrame(results_flattened)\n",
    "\n",
    "# Write results to CSV file\n",
    "results_df.to_csv(\"metrics_results2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
